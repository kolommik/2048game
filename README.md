# 2048 Game - Reinforcement Learning Project

## Описание

Учебный проект для изучения Reinforcement Learning на примере игры 2048. Включает в себя движок игры, различные базовые агенты и интерфейс для визуализации.

## Компоненты

### 1. Game Engine (`game2048_engine.py`)

Основной движок игры с полной реализацией логики 2048:

- **Класс Game2048**:
  - Матрица 4x4 для игрового поля
  - Движения в 4 направлениях (UP, DOWN, LEFT, RIGHT)
  - Детерминированный режим (с seed) для воспроизводимости
  - Методы для копирования состояния (для симуляций)
  - Проверка окончания игры и доступных ходов

**Основные методы:**

- `reset()` - начать новую игру
- `move(direction)` - сделать ход
- `get_available_moves()` - получить список валидных ходов
- `get_state()` / `set_state()` - сохранение/восстановление состояния
- `copy()` - создать копию игры для симуляций

### 2. Агенты (`agents_2048.py`)

Базовый класс `Agent2048` и несколько реализаций:

#### RandomAgent

- Выбирает случайный валидный ход
- Baseline для сравнения
- Средний счет: ~1100

#### GreedyAgent

- Выбирает ход с максимальным немедленным выигрышем
- Простая жадная стратегия
- Средний счет: ~4400

#### CornerAgent

- Старается держать максимальную плитку в углу
- Использует приоритеты направлений
- Средний счет: ~2200

#### MonotonicAgent

- Старается поддерживать монотонность рядов и столбцов
- Использует несколько эвристик:
  - Монотонность (упорядоченность значений)
  - Гладкость (минимизация разницы между соседями)
  - Количество пустых клеток
- Средний счет: ~5000
- **Лучший из базовых агентов**

### 3. Терминальный интерфейс (`terminal_2048.py`)

Интерактивный интерфейс для:

- Ручной игры (WASD управление)
- Наблюдения за игрой агентов
- Быстрого сравнения агентов

**Функции:**

- Цветная визуализация плиток
- Отображение счета и статистики
- Возможность Undo (отмена хода)
- Регулируемая скорость просмотра игры агента

## Использование

### Запуск движка напрямую

```python
from game2048_engine import Game2048, Direction

# Создать игру
game = Game2048(seed=42)  # seed для воспроизводимости

# Сделать ход
valid, points = game.move(Direction.UP)

# Получить доступные ходы
moves = game.get_available_moves()

# Проверить состояние
if game.game_over:
    print(f"Game Over! Score: {game.score}")
```

### Использование агентов

```python
from agents_2048 import MonotonicAgent
from game2048_engine import Game2048

agent = MonotonicAgent()
game = Game2048()

# Играть одну партию
result = agent.play_game(game)
print(f"Score: {result['final_score']}")
print(f"Max tile: {result['max_tile']}")
```

### Терминальный интерфейс

```bash
# Запустить главное меню
python terminal_2048.py

# Играть вручную
python terminal_2048.py play

# Наблюдать за агентом
python terminal_2048.py watch

# Быстрое сравнение агентов
python terminal_2048.py compare
```

## Результаты базовых агентов

На основе 50 игр с детерминированными seed'ами:

| Агент                 | Средний счет | Лучший счет | Лучшая плитка |
| --------------------- | ------------ | ----------- | ------------- |
| Random                | 1,163        | 2,904       | 256           |
| Corner (top-left)     | 2,159        | 5,348       | 512           |
| Corner (bottom-right) | 2,245        | 4,440       | 256           |
| Greedy                | 4,430        | 17,556      | 1024          |
| **Monotonic**         | **4,997**    | **18,408**  | **1024**      |

## Следующие шаги (Roadmap)

### Фаза 2: Gym Environment + DQN

- [ ] Создать Gym-совместимый environment
- [ ] Реализовать DQN агента
- [ ] Эксперименты с разными представлениями состояния
- [ ] Reward shaping

### Фаза 3: Оптимизация и параллелизация

- [ ] Векторизованная версия среды
- [ ] Параллельная генерация опыта
- [ ] Experience replay buffer
- [ ] Улучшенные алгоритмы (Rainbow DQN, PPO)

### Фаза 4: Веб-интерфейс

- [ ] React/Vue фронтенд
- [ ] WebSocket для real-time визуализации
- [ ] Дашборд с графиками обучения
- [ ] Сравнение моделей side-by-side

### Фаза 5: Эксперименты

- [ ] N-tuple networks
- [ ] Monte Carlo Tree Search
- [ ] Transfer learning (разные размеры доски)
- [ ] Интерпретируемость решений

## Производительность

- **Random Agent**: ~0.02 сек/игра
- **Corner Agent**: ~0.03 сек/игра
- **Greedy Agent**: ~0.14 сек/игра
- **Monotonic Agent**: ~0.27 сек/игра

Движок оптимизирован для скорости с использованием NumPy операций.

## Автор

Учебный проект для изучения Reinforcement Learning.
